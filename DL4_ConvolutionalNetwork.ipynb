{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7xxPApuL9Tx8BvyfeO+v7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laxmi-404/Deep-Learning/blob/main/DL4_ConvolutionalNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "One tyhing to realise is that no matter how much hyperparameters you are using,it stops performing beyong 57% accuracy with some simple neural network layers on CIFAR-10.\n",
        "\n",
        "It is possible to make a anetwork with more hidden layers but that is going to be slow and take large amount of RAM.(withn small amount of time).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uX4s7vGUn309"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6-q_doiWQfE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A tarfile is a file format used to store multiple files and directories in a single file, often referred to as a \"tarball.\" It's a type of archive file, commonly used in Unix-based systems (like Linux and macOS) for compressing and bundling files. The tar command (short for \"tape archive\") is used to create and extract these archive files.\n",
        "\n",
        "\n",
        "You can create a .tar file, or combine it with compression methods like gzip (.tar.gz or .tgz) or bzip2 (.tar.bz2), to save space."
      ],
      "metadata": {
        "id": "esMzZcuSqAjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url=\"https://files.fast.ai/data/cifar10.tgz\"\n",
        "# dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\""
      ],
      "metadata": {
        "id": "xYuWKKw1pVjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root= r'C:\\Users\\Admin\\OneDrive\\Desktop\\DEEP_LEARNING'\n",
        "download_url(dataset_url,root=root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r6Ms_BuqVXN",
        "outputId": "fa517b06-2418-4b5f-e327-ab3c15e8a5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: C:\\Users\\Admin\\OneDrive\\Desktop\\DEEP_LEARNING/cifar10.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract from archive:\n",
        "with tarfile.open(r'C:\\Users\\Admin\\OneDrive\\Desktop\\DEEP_LEARNING/cifar10.tgz','r:gz') as tar:\n",
        "  tar.extractall(path=root)"
      ],
      "metadata": {
        "id": "__XuoU7mswvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tarfile.open(file_path, 'r:gz'):\n",
        "\n",
        "'r:gz' indicates you're opening the file in read mode (r) with gzip compression (gz).\n",
        "tar.extractall(path='path_to_extract_directory'):\n",
        "\n",
        "This extracts all files from the archive to the specified directory (path_to_extract_directory).\n",
        "\n",
        "The gzip module in Python provides a way to compress and decompress files using the Gzip compression algorithm, which is commonly used for compressing files (like .gz files) in Unix-like systems."
      ],
      "metadata": {
        "id": "muGwGHzethQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL:\n",
        "The dataset is extracted to directory root.It contains 2 folders(train and test).Training set:50000 images,test set:10000 images.Each of them contains 10 folders one of each class of image."
      ],
      "metadata": {
        "id": "1jJbzQp3zCPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_dir=root+'/cifar10'\n",
        "data_dir=r'C:\\Users\\Admin\\OneDrive\\Desktop\\DEEP_LEARNING/cifar10'\n",
        "print(os.listdir(data_dir))\n",
        "# str of train folder in cifar10\n",
        "classes=os.listdir(data_dir+'/train')\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "3R5Q5qaItit3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221a01e5-06c9-4f7f-b66a-65dc76e0e38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test', 'train']\n",
            "['bird', 'airplane', 'horse', 'frog', 'cat', 'automobile', 'truck', 'deer', 'ship', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "airplane_files=os.listdir(data_dir+'/train/airplane')\n",
        "print('No. of training examples for airplanes:',len(airplane_files))\n",
        "print(airplane_files[:5])"
      ],
      "metadata": {
        "id": "ymJGoemS09eP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f01a3f3-068a-4abb-8449-91b600a00c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples for airplanes: 5000\n",
            "['3796.png', '2431.png', '3772.png', '3041.png', '1112.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ship_test_files=os.listdir(data_dir+'/test/ship')\n",
        "print('No of test examples for ship:',len(ship_test_files))\n",
        "print(ship_test_files[:5])"
      ],
      "metadata": {
        "id": "HuBuW-Cj1Tom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca74d5f3-50a5-450b-f999-9f4a28a8e745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of test examples for ship: 1000\n",
            "['0158.png', '0034.png', '0292.png', '0949.png', '0010.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above directory structure (one folder per class) is used by many computer vision datasets,most DL library provide utility to work with such datasets.We can use ImageFolder class from torchvision to load the data as Pytorch tensor."
      ],
      "metadata": {
        "id": "gORghxaD2NDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "Ep9XIUPW2CXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=ImageFolder(data_dir+'/train',transform=ToTensor())"
      ],
      "metadata": {
        "id": "3TtJdksK2zLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at sample element from training dataset.each element is tuple containing a image tensor and a label.Since the data consist of 32*32 px color image with 3 channel(RGB with image tensor shape(3,32,32)"
      ],
      "metadata": {
        "id": "T239VqyK3ZvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img,label=dataset[0]\n",
        "print(img.shape,label)\n",
        "img\n",
        "# RGB values"
      ],
      "metadata": {
        "id": "SlIZ10Iw3Hao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bad751a-446f-41ef-c41e-ef9108ce0037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32]) 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.7922, 0.7922, 0.8000,  ..., 0.8118, 0.8039, 0.7961],\n",
              "         [0.8078, 0.8078, 0.8118,  ..., 0.8235, 0.8157, 0.8078],\n",
              "         [0.8235, 0.8275, 0.8314,  ..., 0.8392, 0.8314, 0.8235],\n",
              "         ...,\n",
              "         [0.8549, 0.8235, 0.7608,  ..., 0.9529, 0.9569, 0.9529],\n",
              "         [0.8588, 0.8510, 0.8471,  ..., 0.9451, 0.9451, 0.9451],\n",
              "         [0.8510, 0.8471, 0.8510,  ..., 0.9373, 0.9373, 0.9412]],\n",
              "\n",
              "        [[0.8000, 0.8000, 0.8078,  ..., 0.8157, 0.8078, 0.8000],\n",
              "         [0.8157, 0.8157, 0.8196,  ..., 0.8275, 0.8196, 0.8118],\n",
              "         [0.8314, 0.8353, 0.8392,  ..., 0.8392, 0.8353, 0.8275],\n",
              "         ...,\n",
              "         [0.8510, 0.8196, 0.7608,  ..., 0.9490, 0.9490, 0.9529],\n",
              "         [0.8549, 0.8471, 0.8471,  ..., 0.9412, 0.9412, 0.9412],\n",
              "         [0.8471, 0.8431, 0.8471,  ..., 0.9333, 0.9333, 0.9333]],\n",
              "\n",
              "        [[0.7804, 0.7804, 0.7882,  ..., 0.7843, 0.7804, 0.7765],\n",
              "         [0.7961, 0.7961, 0.8000,  ..., 0.8039, 0.7961, 0.7882],\n",
              "         [0.8118, 0.8157, 0.8235,  ..., 0.8235, 0.8157, 0.8078],\n",
              "         ...,\n",
              "         [0.8706, 0.8392, 0.7765,  ..., 0.9686, 0.9686, 0.9686],\n",
              "         [0.8745, 0.8667, 0.8627,  ..., 0.9608, 0.9608, 0.9608],\n",
              "         [0.8667, 0.8627, 0.8667,  ..., 0.9529, 0.9529, 0.9529]]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list of classes  is stored in the .classes property of the dataset.the numeric label for each element corresponds to index of the elements label in the list of classes."
      ],
      "metadata": {
        "id": "qT9-zRlp4Wl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.classes)"
      ],
      "metadata": {
        "id": "dN8I5SHQ4KnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2487ed7e-d95f-4fe4-f32a-9f13377d1653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can view the image using matplolib,but we need to change the tensor dimension (32,32,3)"
      ],
      "metadata": {
        "id": "o5vTjYae481l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def show_example(img,label):\n",
        "  print('label:',dataset.classes[label],\"(\"+str(label)+\")\")\n",
        "  plt.imshow(img.permute(1,2,0))"
      ],
      "metadata": {
        "id": "_h9JG29R48fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img,label=dataset[0]\n",
        "show_example(img,label)"
      ],
      "metadata": {
        "id": "eGPsHsa-4sGp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "912c78a9-8dca-4ab6-ff7f-1aba7afdf0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: airplane (0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_example(*dataset[1099])"
      ],
      "metadata": {
        "id": "Ft74NZ7H4r7U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "819e6c4b-67e6-4925-a99b-36892cfac554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: airplane (0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The asterisk (*) to unpack or reference an item from a dataset. It seems like you want to display an example from a dataset, specifically the one at index 1099."
      ],
      "metadata": {
        "id": "9ZQKRi8sb4WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_example(*dataset[10099])"
      ],
      "metadata": {
        "id": "jbLfziLlb5NI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "a07a2a4a-f812-4502-a4de-c6dccbed69a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: bird (2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splits of dataset:\n",
        "\n",
        "Training set - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n",
        "\n",
        "Validation set - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n",
        "\n",
        "Test set - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model."
      ],
      "metadata": {
        "id": "U4r8HKXPcrIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed=42\n",
        "torch.manual_seed(random_seed)"
      ],
      "metadata": {
        "id": "eOB5Ru-wcOfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b1ee5e-d450-48ff-c2cc-f87da7edc0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x791e64721ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code torch.manual_seed(random_seed) is used in PyTorch to set the seed for generating random numbers, ensuring reproducibility. By setting a specific random_seed, you make sure that any random operations (like initializing model weights, shuffling data, etc.) will produce the same result each time you run your code, which is helpful for debugging and experiments."
      ],
      "metadata": {
        "id": "KL8S38MQdZ9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_size=5000\n",
        "train_size=len(dataset)-val_size\n",
        "\n",
        "train_ds,val_ds=random_split(dataset,[train_size,val_size])\n",
        "len(train_ds),len(val_ds)"
      ],
      "metadata": {
        "id": "aL16a6LKda0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db26bd6-973e-4a01-c0e6-ba9998c3ee24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "metadata": {
        "id": "-bOqI_ytd2Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=128"
      ],
      "metadata": {
        "id": "XoUNdQXzeD3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl=DataLoader(train_ds,batch_size,shuffle=True,num_workers=2,pin_memory=True)\n",
        "val_dl=DataLoader(val_ds,batch_size*2,num_workers=2,pin_memory=True)"
      ],
      "metadata": {
        "id": "oqrEHzp0eHB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at batches of images from the dataset using the make_grid method from torchvision. Each time the following code is run, we get a different bach, since the sampler shuffles the indices before creating batches."
      ],
      "metadata": {
        "id": "oSIxHWy-ekgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, setting shuffle=True is often a good practice when tuning hyperparameters, particularly during training, because it ensures that the data is presented in a random order for each epoch"
      ],
      "metadata": {
        "id": "eanYhS5ffUFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import make_grid\n",
        "def show_batch(dl):\n",
        "  for images,labels in dl:\n",
        "    fig,ax=plt.subplots(figsize=(12,6))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
        "    break"
      ],
      "metadata": {
        "id": "ah6DKRQGeZZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "ax as the container for the image:\n",
        "The key reason ax is used here is that Matplotlib works by placing elements (like images, lines, or annotations) inside an \"axes\" container. ax represents the plot area where you add the image and other visual elements. Without it, you wouldn't have a way to control or customize the layout of your plot."
      ],
      "metadata": {
        "id": "ryw95vNIgV8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We defined a deep neural network using nn.Linear."
      ],
      "metadata": {
        "id": "2q9ps6vohQCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output **pixel**\n",
        "\n",
        "we will use a convolutional neural network, using the nn.Conv2d class from PyTorch.\n",
        "\n",
        "https://medium.com/towards-data-science/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n",
        "\n",
        "The size of the kernel directly determines how many (or few) input features get combined in the production of a new output feature.\n",
        "\n",
        "Padding: If you see the animation above, notice that during the sliding process, the edges essentially get “trimmed off”, converting a 5×5 feature matrix to a 3×3 one. The pixels on the edge are never at the center of the kernel, because there is nothing for the kernel to extend to beyond the edge. This isn’t ideal, as often we’d like the size of the output to equal the input.\n",
        "\n",
        "Padding does something pretty clever to solve this: pad the edges with extra, “fake” pixels (usually of value 0, hence the oft-used term “zero padding”). This way, the kernel when sliding can allow the original edge pixels to be at its center, while extending into the fake pixels beyond the edge, producing an output the same size as the input.\n",
        "\n",
        "Striding: Often when running a convolution layer, you want an output with a lower size than the input. This is commonplace in convolutional neural networks, where the size of the spatial dimensions are reduced when increasing the number of channels. One way of accomplishing this is by using a pooling layer (eg. taking the average/max of every 2×2 grid to reduce each spatial dimensions in half). Yet another way to do is is to use a stride:\n",
        "\n",
        "The idea of the stride is to skip some of the slide locations of the kernel. A stride of 1 means to pick slides a pixel apart, so basically every single slide, acting as a standard convolution. A stride of 2 means picking slides 2 pixels apart, skipping every other slide in the process, downsizing by roughly a factor of 2, a stride of 3 means skipping every 2 slides, downsizing roughly by factor 3, and so on.\n",
        "\n",
        "More modern networks, such as the ResNet architectures entirely forgo pooling layers in their internal layers, in favor of strided convolutions when needing to reduce their output sizes.\n",
        "\n"
      ],
      "metadata": {
        "id": "J593nikQhY0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "So this is where a key distinction between terms comes in handy: whereas in the 1 channel case, where the term filter and kernel are interchangeable, in the general case, they’re actually pretty different. Each filter actually happens to be a collection of kernels, with there being one kernel for every single input channel to the layer, and each kernel being unique.\n",
        "\n",
        "Each filter in a convolution layer produces one and only one output channel, and they do it like so:\n",
        "\n",
        "Each of the kernels of the filter “slides” over their respective input channels, producing a processed version of each. Some kernels may have stronger weights than others, to give more emphasis to certain input channels than others (eg. a filter may have a red kernel channel with stronger weights than others, and hence, respond more to differences in the red channel features than the others).\n",
        "\n",
        "\n",
        "Each of the per-channel processed versions are then summed together to form one channel. The kernels of a filter each produce one version of each channel, and the filter as a whole produces one overall output channel.\n",
        "\n"
      ],
      "metadata": {
        "id": "FLac1h42UVIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_kernel(image,kernel):\n",
        "  ri,ci=image.shape\n",
        "  rk,ck=kernel.shape\n",
        "  ro,co=ri-rk+1,ci-ck+1\n",
        "  output=torch.zeros([ro,co])\n",
        "  for i in range(ro):\n",
        "    for j in range(co):\n",
        "      output=torch.sum(image[i:i+rk,j:j+ck]*kernel)\n",
        "    return output"
      ],
      "metadata": {
        "id": "a78iXa7mgW0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " output=torch.zeros([ro,co]):\n",
        "\n",
        "This can be useful if you're initializing a tensor to store some results (like matrix operations, output of a model, etc.) but don't have values for it yet and want to fill it with zeros initially."
      ],
      "metadata": {
        "id": "XcRyIofYXiUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image=torch.tensor([\n",
        "    [3,3,2,1,0],\n",
        "    [0,0,1,3,1],\n",
        "    [3,1,2,2,3],\n",
        "    [2,0,0,2,2],\n",
        "    [2,0,0,0,1]\n",
        "],dtype='float32'\n",
        ")"
      ],
      "metadata": {
        "id": "AvliR7lfX-dS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e6420d25-1d82-4a9f-cf04-ec7ea215a081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "tensor(): argument 'dtype' must be torch.dtype, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-7c43b46efb5b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m sample_image=torch.tensor([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tensor(): argument 'dtype' must be torch.dtype, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_kernel=torch.tensor([\n",
        "    [0,1,2],\n",
        "    [2,2,0],\n",
        "    [0,1,2]\n",
        "],dtype='float32'\n",
        ")"
      ],
      "metadata": {
        "id": "GoQe5ngjYh8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apply_kernel(sample_image,sample_kernel)"
      ],
      "metadata": {
        "id": "tQSYQfkhYpM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By other parameters,kernels(weights randomly initialised) gets better and better."
      ],
      "metadata": {
        "id": "jcEPQg30Z8em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are certain advantages offered by convolutional layers when working with image data:\n",
        "\n",
        "Fewer parameters: A small set of parameters (the kernel) is used to calculate outputs of the entire image, so the model has much fewer parameters compared to a fully connected layer.\n",
        "\n",
        "Sparsity of connections: In each layer, each output element only depends on a small number of input elements, which makes the forward and backward passes more efficient.\n",
        "\n",
        "Parameter sharing and spatial invariance: The features learned by a kernel in one part of the image can be used to detect similar pattern in a different part of another image.\n",
        "\n",
        "We will also use a max-pooling layers to progressively decrease the height & width of the output tensors from each convolutional layer."
      ],
      "metadata": {
        "id": "qJ-pbyUfaely"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "Max pooling is a downsampling operation commonly used in Convolutional Neural Networks (CNNs). It reduces the spatial dimensions of an input (such as an image or feature map) by taking the maximum value from a specific region of the input. This operation helps retain the most important features and reduces computational load."
      ],
      "metadata": {
        "id": "HzSJNCetbDdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "LacrJzHGYvQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model=nn.Sequential(\n",
        "    nn.Conv2d(3,8,kernel_size=3,stride=1,padding=1),\n",
        "    nn.MaxPool(2,2)\n",
        ")"
      ],
      "metadata": {
        "id": "P26wdRnIbMtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Sequential: give many layers\n",
        "\n",
        "Each of the 8 filters applies across all 3 channels (RGB), so the kernel for each filter will have dimensions [3, 3, 3]:\n",
        "\n",
        "3 corresponds to the number of input channels (RGB).\n",
        "3x3 is the size of the filter applied spatially.\n",
        "For each filter:\n",
        "\n",
        "The kernel will be applied to the 3 input channels.\n",
        "For each 3x3 region of the input, the filter computes a weighted sum of the values across all 3 channels and produces a single output value.\n",
        "\n",
        "After applying this to the entire image, you get one output feature map per filter.\n",
        "\n",
        "Since there are 8 filters, you will get 8 output feature maps (one per filter), and these maps will form the 8 output channels.\n",
        "\n",
        "The stride of 1 means that the filter moves by one unit at a time across the image.\n",
        "\n",
        "Padding of 1 ensures that the spatial dimensions (height and width) of the output are the same as the input (when the kernel size is 3, padding of 1 keeps the input size the same as the output size)."
      ],
      "metadata": {
        "id": "81H1lbbpeOd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images,labels in train_dl:\n",
        "  print('images.shape',images.shape)\n",
        "  out=simple_model(images)\n",
        "  print('out.shape',out.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "J1z_okMVbt3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Conv2d layer transforms a 3-channel image to a 16-channel feature map, and the MaxPool2d layer halves the height and width. The feature map gets smaller as we add more layers, until we are finally left with a small feature map, which can be flattened into a vector. We can then add some fully connected layers at the end to get vector of size 10 for each image"
      ],
      "metadata": {
        "id": "E3viHxX3hB1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def accuracy(outputs,labels):\n",
        "  _,preds=torch.max(outputs,dim=1)\n",
        "  return torch.tensor(torch.sum(preds==labels).item()/len(preds))\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "  def training_step(self,batch):\n",
        "    images,labels=batch\n",
        "    out=self(images)\n",
        "    loss=F.cross_entropy(out,labels)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self,batch):\n",
        "    images,labels=batch\n",
        "    out=self(images)\n",
        "    loss=F.cross_entropy(out,labels)\n",
        "    accuracy=accuracy(out,labels)\n",
        "    return {'val_loss':loss.detach(),'val_accuracy':accuracy}\n",
        "\n",
        "  def validation_epoch_end(self,outputs):\n",
        "    batch_losses=[x['val_loss'] for x in outputs]\n",
        "    epoch_loss=torch.stack(batch_losses).mean()\n",
        "    batch_accuracies=[x['val_accuracy'] for x in outputs ]\n",
        "    epoch_accuracy=torch.stack(batch_accuracies).mean()\n",
        "    return {'val_loss':epoch_losses.item(),'val_accuracy':epoch_accuracy.item()}\n",
        "\n",
        "  def epoch_end(self,epoch,result):\n",
        "    print(\"Epoch [{}],train_loss:{:.4f},val_loss:{:.4f}\".format(epoch,result['train_loss'],result['val_loss']))\n"
      ],
      "metadata": {
        "id": "W_579DE_fm43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of Convolutional Neural Networks (CNNs), increasing the number of channels typically means adding more filters in the convolutional layer. Each filter learns to detect different features in the input image, such as edges, textures, or more complex patterns.\n",
        "\n",
        "Here’s what increasing channels (or filters) means in more detail:\n",
        "\n",
        "1. Channels in the Input (RGB Image Example):\n",
        "For an RGB image, there are 3 input channels: Red, Green, and Blue.\n",
        "Each channel holds pixel values corresponding to the color intensity for that channel across the image.\n",
        "2. Convolutional Layers and Output Channels:\n",
        "When you apply a convolutional layer with multiple filters, each filter produces its own output channel (feature map).\n",
        "The number of output channels is determined by the number of filters in the convolutional layer.\n",
        "If you increase the number of filters, you increase the number of output channels in the feature map.\n",
        "Why Increase the Number of Channels (Filters)?\n",
        "Capture More Features:\n",
        "\n",
        "Each filter detects different features. For example, some filters might learn to detect edges, others might detect textures, and others might capture more complex patterns.\n",
        "Increasing the number of channels (filters) allows the network to capture more complex and diverse features from the input.\n",
        "More Expressive Power:\n",
        "\n",
        "With more filters, the network has more capacity to learn from the data and detect more abstract features at each layer, which can be important for complex tasks (e.g., object recognition).\n",
        "More filters provide more expressive power, allowing the model to learn a richer set of features.\n",
        "Deep Networks:\n",
        "\n",
        "In deep CNN architectures, it is common to see the number of channels increase as you move deeper into the network. Early layers might use a smaller number of filters (e.g., 32 or 64), while deeper layers may use a larger number of filters (e.g., 256, 512, etc.).\n",
        "The intuition is that as you go deeper, the model can capture higher-level, more abstract features, so the network needs more channels to represent this complexity.\n",
        "Example: Increasing Channels in a CNN\n",
        "Consider a simple CNN with 3 layers. Let's examine how the number of channels can increase as we go deeper into the network:\n",
        "\n",
        "Input: Let's assume the input is a color image with 3 channels (RGB), and its size is 32x32 pixels.\n",
        "\n",
        "First Convolutional Layer:\n",
        "\n",
        "Let's use 16 filters in the first convolutional layer. Each filter will produce one output channel.\n",
        "After applying the filters, the output will have 16 channels (one for each filter).\n",
        "The spatial size of the image might stay the same (e.g., 32x32) if we use padding to preserve the dimensions.\n",
        "Second Convolutional Layer:\n",
        "\n",
        "The second layer could have 32 filters. Now, we’re working with an input of 16 channels (from the previous layer).\n",
        "The output of this layer will have 32 channels, and again, the spatial size can be kept the same using padding.\n",
        "Third Convolutional Layer:\n",
        "\n",
        "The third layer could have 64 filters, so the output will have 64 channels."
      ],
      "metadata": {
        "id": "zqG8coJrjs_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "Adding non-linearity (typically using activation functions like ReLU, Sigmoid, Tanh, etc.) after each layer in a neural network is crucial for the model’s ability to learn and represent complex patterns. Here's why it \"powers\" the model:\n",
        "\n",
        "1. Breaking Linearity (Enabling Complex Mapping):\n",
        "If you didn't add non-linear activation functions, your network would just be a series of linear transformations.\n",
        "\n",
        "A linear transformation (such as a matrix multiplication or a convolution) maps input to output through simple weighted sums. Even if you stacked many layers, the overall function would still be linear.\n",
        "\n",
        "Mathematically, if you have:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝑊\n",
        "2\n",
        "(\n",
        "𝑊\n",
        "1\n",
        "𝑥\n",
        ")\n",
        "y=W\n",
        "2\n",
        "​\n",
        " (W\n",
        "1\n",
        "​\n",
        " x)\n",
        "where\n",
        "𝑊\n",
        "1\n",
        "W\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑊\n",
        "2\n",
        "W\n",
        "2\n",
        "​\n",
        "  are weight matrices, the entire operation remains a linear function of\n",
        "𝑥\n",
        "x.\n",
        "\n",
        "Without non-linearity, no matter how deep your network is, it would only be able to represent linear functions, limiting its power. For example, even with many layers, it could only draw straight lines or hyperplanes and would struggle to model more complex data like images or natural language.\n",
        "\n",
        "2. Learning Complex, High-Dimensional Functions:\n",
        "When you add a non-linear activation like ReLU, Sigmoid, or Tanh, each layer can learn more complex relationships in the data.\n",
        "Non-linearity allows the network to approximate complex functions. It lets the model learn intricate patterns, such as curves or other complex behaviors, which are essential for tasks like image classification, speech recognition, or machine translation.\n",
        "3. Universal Approximation Theorem:\n",
        "A key result in neural networks is the Universal Approximation Theorem, which states that a feedforward neural network with at least one hidden layer, non-linear activation functions, and enough units can approximate any continuous function (with sufficient precision).\n",
        "This means that by adding non-linearity, the model has the ability to represent any complex pattern in the data, no matter how complicated.\n",
        "4. Intuition Behind Non-Linearity:\n",
        "Here’s an intuitive explanation:\n",
        "\n",
        "Suppose you want to separate a set of points that form a circle in 2D space (e.g., classifying points inside and outside the circle). A straight line (linear classifier) can’t do this, but a neural network with non-linear activations can!\n",
        "The network can learn to separate points in non-linear ways, like using a curved boundary, which is necessary for many real-world tasks"
      ],
      "metadata": {
        "id": "m3Y3LJfakVEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use nn.Sequential to chain activation functions and single network architecture."
      ],
      "metadata": {
        "id": "JjR5A6G9D4Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cifar10CnnModel(ImageClassificationBase):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.network=nn.Sequential(\n",
        "        nn.Conv2d(3,32,kernel_size=3,padding=1),\n",
        "        nn.ReLu(),\n",
        "        nn.Conv2d(32,64,kernel_size=3,padding=1),\n",
        "        nn.ReLu(),\n",
        "        nn.MaxPool2d(2,2)\n",
        "        # output is 64X16X16\n",
        "\n",
        "        nn.Conv2d(64,128,kernel_size=3,padding=1);\n",
        "        nn.ReLu()\n",
        "        nn.Conv2d(128,128,kernel_size=3,padding=1),\n",
        "        nn.ReLu()\n",
        "        nn.MaxPool2d(2,2)\n",
        "        # output is (128*8*8)\n",
        "\n",
        "        nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
        "        nn.Relu(),\n",
        "        nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1)\n",
        "        nn.ReLu()\n",
        "        nn.MaxPool(2,2)\n",
        "        # output is 256*4*4\n",
        "\n",
        "        nn.Flatten()\n",
        "        nn.Linear(256*4*4,1024),\n",
        "        nn.ReLu(),\n",
        "        nn.Linear(1024,512),\n",
        "        nn.ReLu(),\n",
        "        nn.Linear(512,10)\n",
        "    )\n",
        "\n",
        "    def forward(self,xb):\n",
        "      return self.network(xb)\n",
        "\n",
        "      # self.network is a sequential module."
      ],
      "metadata": {
        "id": "zQXrz07Vju6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Your Code\n",
        "\n",
        "```python\n",
        "def __init__(self):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=3, padding=1),  # 1st Conv Layer\n",
        "        nn.ReLU(),  # ReLU Activation\n",
        "        nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 2nd Conv Layer\n",
        "        nn.ReLU(),  # ReLU Activation\n",
        "        nn.MaxPool2d(2, 2)  # Max Pooling\n",
        "    )\n",
        "```\n",
        "\n",
        "### Breaking It Down\n",
        "\n",
        "1. **Input Size**:\n",
        "   - The input size is typically assumed to be something like **3 x 32 x 32**, where:\n",
        "     - **3**: Number of channels (e.g., for RGB images).\n",
        "     - **32**: Height and width of the image.\n",
        "\n",
        "2. **First Convolutional Layer**: `nn.Conv2d(3, 32, kernel_size=3, padding=1)`\n",
        "   - This layer takes an input with **3 channels** (e.g., RGB image) and applies **32 filters** of size **3x3**.\n",
        "   - The **padding=1** means we add one pixel padding on all sides of the input image, keeping the spatial dimensions the same.\n",
        "   - The output dimensions will be the same as the input, i.e., the height and width remain **32**. After this convolution, the output will have **32 channels** (one for each filter), and the shape of the output will be **32 x 32 x 32**.\n",
        "\n",
        "3. **ReLU Activation**: `nn.ReLU()`\n",
        "   - The ReLU activation function doesn't change the dimensions, it simply applies a non-linear transformation. The output remains **32 x 32 x 32**.\n",
        "\n",
        "4. **Second Convolutional Layer**: `nn.Conv2d(32, 64, kernel_size=3, padding=1)`\n",
        "   - This layer takes the **32 channels** from the previous layer and applies **64 filters** of size **3x3**.\n",
        "   - The **padding=1** again ensures that the spatial dimensions (height and width) are preserved.\n",
        "   - The output dimensions will still be **32 x 32** (height and width), but now there are **64 channels**. So, after this convolution, the output will be **64 x 32 x 32**.\n",
        "\n",
        "5. **ReLU Activation**: `nn.ReLU()`\n",
        "   - As before, ReLU just applies a non-linear activation function, keeping the output dimensions the same at **64 x 32 x 32**.\n",
        "\n",
        "6. **Max Pooling Layer**: `nn.MaxPool2d(2, 2)`\n",
        "   - The **Max Pooling** operation with a **kernel size of 2** and **stride of 2** reduces the spatial dimensions by a factor of 2 (both height and width).\n",
        "   - The output size after max pooling can be calculated as:\n",
        "     \\[\n",
        "     \\text{Output size} = \\frac{\\text{Input size}}{\\text{Stride}} = \\frac{32}{2} = 16\n",
        "     \\]\n",
        "   - Therefore, after applying max pooling, the spatial dimensions (height and width) are reduced to **16 x 16**, and the number of channels remains **64**.\n",
        "   - The output of this layer will be **64 x 16 x 16**.\n",
        "\n",
        "### Final Output Dimensions\n",
        "\n",
        "- After all the layers, the output tensor has dimensions of **64 x 16 x 16**, where:\n",
        "  - **64** is the number of channels (due to the 64 filters in the second convolutional layer).\n",
        "  - **16 x 16** is the reduced spatial size (height and width) after the convolution layers and the max pooling operation.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "- The input image (assumed **3 x 32 x 32**) undergoes two convolutional layers, each followed by a ReLU activation.\n",
        "- After the first convolution, the output has **32 channels** with spatial dimensions of **32 x 32**.\n",
        "- After the second convolution, the output has **64 channels** with spatial dimensions still **32 x 32**.\n",
        "- Finally, after the max pooling operation, the spatial dimensions are halved to **16 x 16**, resulting in an output of **64 x 16 x 16**.\n",
        "\n"
      ],
      "metadata": {
        "id": "_NvtwL04HgMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------\n",
        "\n",
        "The actual process of selecting and refining kernels happens during the training phase, specifically through the backpropagation algorithm and gradient descent.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Forward Pass:\n",
        "\n",
        "The input image (or feature map) is passed through the CNN layers, where each layer applies its respective kernels to extract features.\n",
        "Initially, the kernels produce arbitrary feature maps since they were initialized randomly.\n",
        "Loss Calculation:\n",
        "\n",
        "The output of the network is compared with the target label (for example, a class label in classification tasks).\n",
        "A loss function (e.g., cross-entropy loss for classification) is calculated, which measures how well the network's output matches the desired result.\n",
        "\n",
        "Backpropagation:\n",
        "\n",
        "The backpropagation algorithm computes the gradient of the loss function with respect to the kernel values. This tells the network how to adjust the kernels to minimize the loss.\n",
        "\n",
        "Essentially, the network \"learns\" which kernels (filters) help reduce the error and improve its predictions.\n",
        "Gradient Descent Update:\n",
        "\n",
        "Using an optimization technique like gradient descent (or variants like Adam, SGD, etc.), the network updates the kernel values. The gradients indicate how the kernels should be modified to reduce the loss.\n",
        "\n",
        "The weights (values of the kernels) are adjusted to improve the network’s performance on the task.\n",
        "Repetition:\n",
        "\n",
        "This process repeats for each batch of training data, allowing the network to gradually refine the kernels over many iterations (epochs).\n",
        "\n",
        "As the kernels are updated over time, they learn to detect more useful and abstract features, such as edges, textures, and eventually high-level features like shapes and objects."
      ],
      "metadata": {
        "id": "tSy8-LHe_2G8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "Convolutional Layers: Learn spatial features (edges, textures, objects, etc.).\n",
        "\n",
        "Example: After several convolutional layers, you might have learned features like \"horizontal edge,\" \"vertical edge,\" \"red color,\" or \"round shape\" in an image.\n",
        "\n",
        "Flattening: After convolutional layers, the output is typically in the form of a multi-dimensional tensor (height, width, and depth). Before passing this output to a linear layer, it is often flattened into a 1D vector. This flattening process turns the 2D feature maps (the output of the convolutional layers) into a 1D vector of features that the linear layer can use.\n",
        "\n",
        "Linear Layer: The linear layer then takes this vector of features and processes them in a way that allows the model to make a final decision (such as classifying an image into one of several categories)."
      ],
      "metadata": {
        "id": "LzmQiQ6EL7ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Cifar10CnnModel()\n",
        "model"
      ],
      "metadata": {
        "id": "VtciQhxB_9TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify that the model produces the expected output on a batch of training data. The 10 outputs for each image can be interpreted as probabilities for the 10 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image."
      ],
      "metadata": {
        "id": "uigmHeb5M4o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images,labels in train_dl:\n",
        "  print('images.shape',images.shape)\n",
        "  out=model(images)\n",
        "  print('out shape:',out.shape)\n",
        "  print('out[0]',out[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "ZUfBSMwkMYsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "  # pick GPU else CPU\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def to_device(data,device):\n",
        "  # Move tensor to chosen device\n",
        "  if isinstance(data,(list,tuple)):\n",
        "    return [to_device(x,device) for x in data]\n",
        "  return daat.to_device(device,non_blocking=True)\n",
        "\n",
        "def DeviceDataLoader():\n",
        "  # Wrap a dataloader to move data to device\n",
        "  def __init__(self,dl,device):\n",
        "    self.dl=dl\n",
        "    self.device=device\n",
        "\n",
        "# Yield a batch of data after moving to device\n",
        "  def __iter__(self):\n",
        "    for b in self.dl:\n",
        "      yield to_device(b,self.device)\n",
        "#Count the no of batches\n",
        "  def __len__(self):\n",
        "    return len(self.dl)"
      ],
      "metadata": {
        "id": "c4YFUj_ENNtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training neural networks, you often have to deal with batches of data (e.g., a batch of images or a batch of input features). These batches could be in the form of:\n",
        "\n",
        "A list or tuple containing multiple individual tensors, such as different parts of the input data (e.g., image data and labels).\n",
        "\n",
        "\n",
        "A single tensor representing the whole batch.\n",
        "In such cases, the to_device function recursively checks if data is a list or tuple.\n",
        "\n",
        "If it is, it applies the same to_device function to each element inside the list or tuple. This ensures that all the tensors in the list or tuple are moved to the same device (CPU or GPU)."
      ],
      "metadata": {
        "id": "c5HP4n3qOWju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=get_default_device()\n",
        "device"
      ],
      "metadata": {
        "id": "XF1ZMv0-PPu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DeviceDataLoader(train_dl,device)\n",
        "val_loader=DeviceDataLoader(val_dl,device)"
      ],
      "metadata": {
        "id": "OhApH0TwPmxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_device(model,device)\n",
        "# pehle wale se cpu mei jaa raha then gpu mei"
      ],
      "metadata": {
        "id": "xjv-vH4uP6w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A feature map is an essential concept in Convolutional Neural Networks (CNNs). It refers to the output of a convolutional layer (or any layer that performs feature extraction) after it has been applied to the input data. Essentially, it's a transformed version of the input data that highlights certain patterns or features."
      ],
      "metadata": {
        "id": "ou3roL4AQ39n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model,val_loader):\n",
        "  model.eval()\n",
        "  outputs=[model.validation_step(batch) for batch in val_loader]\n",
        "  return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs,lr,model,train_loader,val_loader,opt_func=torch.optim.SGD):\n",
        "  history=[]\n",
        "  optimizer=opt_func(model.parameters(),lr)\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_losses=[]\n",
        "    for batch in train_loader:\n",
        "      loss=model.training_step(batch)\n",
        "      train_losses.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    # Validation Phase:\n",
        "    result=evaluate(model,val_loader)\n",
        "    result['train_loss']=torch.stack(train_losses).mean().item()\n",
        "    model.epoch_end(epoch,result)\n",
        "    history.append(result)\n",
        "  return history"
      ],
      "metadata": {
        "id": "bKngchi5Q4w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " @torch.no_grad() decorator in PyTorch is a context manager that disables gradient tracking during the execution of a specific block of code. This is especially useful when you are performing inference (i.e., testing or validation) and do not need to compute gradients, which helps to save memory and improve performance.\n",
        "\n",
        "\n",
        "The model.eval() method in PyTorch is used to set a model into evaluation mode. This is an important part of working with neural networks, especially when you're switching from training mode to inference (testing or validation) mode.\n",
        "\n",
        "In PyTorch, some layers in a neural network, such as Dropout and Batch Normalization, behave differently during training and evaluation.\n",
        "\n",
        "Training Mode: During training, Dropout randomly deactivates certain neurons to prevent overfitting, and Batch Normalization normalizes activations based on the current batch of data.\n",
        "\n",
        "Evaluation Mode: During evaluation, we don’t want Dropout to deactivate any neurons, and we want Batch Normalization to use the running statistics (mean and variance) computed during training, not the statistics of the current batch.\n",
        "\n",
        "By calling model.eval(), you're telling PyTorch to disable Dropout and use the fixed statistics for Batch Normalization during inference.\n",
        "\n",
        "What Happens When You Call model.eval()?\n",
        "Dropout Layers: Dropout is disabled, meaning that no neurons will be randomly deactivated. All neurons will be active.\n",
        "\n",
        "Batch Normalization: Instead of using the statistics of the current batch (which can be noisy), the model will use the running averages of the mean and variance computed during training."
      ],
      "metadata": {
        "id": "hwmRpT-WpRKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Use model.train()?\n",
        "When you're training a model, you want some layers to behave in a specific way:\n",
        "\n",
        "Dropout should be active and randomly deactivate neurons during training to prevent overfitting.\n",
        "\n",
        "Batch Normalization should compute statistics (mean and variance) from the current batch of data, as it helps the model learn the distribution of the data during training."
      ],
      "metadata": {
        "id": "dfG5zFfFrIUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=to_device(Cifar10CnnModel(),device)"
      ],
      "metadata": {
        "id": "efTJ5b5sp_Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model,val_loader)"
      ],
      "metadata": {
        "id": "qRlZq2-Nsk9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=10\n",
        "opt_Func=torch.optim.Adam\n",
        "lr=0.001"
      ],
      "metadata": {
        "id": "jasJe9P3soHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=fit(num_epochs,lr,model,train_loader,val_loader,opt_func)"
      ],
      "metadata": {
        "id": "jXCpmsudtCVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam (Adaptive Moment Estimation) and SGD (Stochastic Gradient Descent) are both optimization algorithms commonly used to train machine learning models, especially in the context of deep learning.\n",
        "\n",
        "Both algorithms aim to minimize the loss function by updating the model's weights during training, but they do so in different ways.\n",
        "\n",
        "How SGD Works:\n",
        "\n",
        "In SGD, we compute the gradient of the loss function with respect to the parameters (weights) using a small batch (or a single data point) at each step. This is in contrast to Batch Gradient Descent, where the gradient is computed over the entire dataset.\n",
        "\n",
        "We update the model parameters after every batch or data point, which is why it's called stochastic.\n",
        "\n",
        "Adam maintains two moving averages for each parameter:\n",
        "First moment (mt): The running average of the gradients.\n",
        "Second moment (vt): The running average of the squared gradients.\n",
        "\n",
        "These moments are used to adjust the learning rate for each parameter individually, making Adam more robust to different types of data and model architectures"
      ],
      "metadata": {
        "id": "T9gX17nhuBYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracies(history):\n",
        "  accuracies=[x['val_accuracy'] for x in history]\n",
        "  plt.plot(accuracies,'-x')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.title('Accuracy vs No of Epoch')"
      ],
      "metadata": {
        "id": "daPL_lqTur3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracies(history)"
      ],
      "metadata": {
        "id": "EJ2Rk434vPqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model reaches an accuracy of around 75%, and by looking at the graph, it seems unlikely that the model will achieve an accuracy higher than 80% even after training for a long time. This suggests that we might need to use a more powerful model to capture the relationship between the images and the labels more accurately. This can be done by adding more convolutional layers to our model, or incrasing the no. of channels in each convolutional layer, or by using regularization techniques."
      ],
      "metadata": {
        "id": "D0FLdwhmviNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(history):\n",
        "  train_losses=[x.get('train_loss') for x in history]\n",
        "  val_losses=[x['val_loss'] for x in history]\n",
        "  plt.plot(train_losses,'-bx')\n",
        "  plt.plot(val_losses,'-rx')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.legend(['Training','Validation'])\n",
        "  plt.title('Loss vs no of Epochs')"
      ],
      "metadata": {
        "id": "bWzNG2FFvZD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get() is safer because if the key is missing, it will return None (or a default value) instead of raising an error.\n",
        "Direct access (x['train_loss']) is faster but will raise an error if the key is missing."
      ],
      "metadata": {
        "id": "66yJHOmOwgJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialy, both the training and validation losses seem to decrease over time. However, if you train the model for long enough, you will notice that the training loss continues to decrease, while the validation loss stops decreasing, and even starts to increase after a certain point!"
      ],
      "metadata": {
        "id": "n0FmVr0-vtFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This phenomenon is called overfitting, and it is the no. 1 why many machine learning models give rather terrible results on real-world data. It happens because the model, in an attempt to minimize the loss, starts to learn patters are are unique to the training data, sometimes even memorizing specific training examples. Because of this, the model does not generalize well to previously unseen data"
      ],
      "metadata": {
        "id": "BUs8Rygax9X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(history)"
      ],
      "metadata": {
        "id": "X0hAzZCpvS_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following are some common stragegies for avoiding overfitting:\n",
        "\n",
        "Gathering and generating more training data, or adding noise to it\n",
        "\n",
        "Using regularization techniques like batch normalization & dropout(nn.dropout(0.4))\n",
        "\n",
        "Early stopping of model's training, when validation loss starts to increase"
      ],
      "metadata": {
        "id": "aRCE2mViyw5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset=ImageFolder(data_dir+'/test',transform=Totensor())"
      ],
      "metadata": {
        "id": "V__yIF7QxLtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(img,model):\n",
        "  # Convert to a batch of 1\n",
        "  xb=to_device(img.unsqueeze(0),device)\n",
        "  # Get prediction from model\n",
        "  yb=model(xb)\n",
        "  # Pick index with highest probability\n",
        "  _,preds=torch.max(yb,dim=1)\n",
        "  # Retrieve the class label\n",
        "  return dataset.classes[preds[0].item()]\n"
      ],
      "metadata": {
        "id": "bSZw6GczzY67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your image has a shape of (channels, height, width), adding the batch dimension will change the shape to (1, channels, height, width), representing a single image in a batch."
      ],
      "metadata": {
        "id": "11sZkFkc0jAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------\n",
        "WHY ARE WE MAKING IT TO BATCH OF 1 NOT 128?\n",
        "\n",
        "When Would You Use Larger Batches?\n",
        "You would use larger batch sizes (e.g., 128) when:\n",
        "\n",
        "You're training the model and want to accelerate the training process by feeding multiple images at once.\n",
        "You're running inference in bulk and have a large dataset or need to process multiple images in parallel.\n",
        "\n",
        "This is most likely for model inference or evaluation on a single image.\n",
        "\n",
        "When you're passing a single image to a model, PyTorch still expects the input to be in the format of a batch, even if that batch contains only one image.\n",
        "\n",
        "Why a batch?: Neural networks are usually designed to process multiple inputs simultaneously, even if you're working with just one. By adding a batch dimension with unsqueeze(0), you're preparing the image in the correct format for the model, which expects an input of shape (batch_size, channels, height, width)."
      ],
      "metadata": {
        "id": "X650gfAY1Sum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "preds[0] extracts the predictions for the first image in the batch.\n",
        "\n",
        ".item() gets the index of the predicted class from those logits or probabilities.\n",
        "\n",
        "dataset.classes[...] maps the class index to the actual class name or label."
      ],
      "metadata": {
        "id": "e4Z_F1bT3TVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "What .item() Does?\n",
        "\n",
        "\n",
        ".item() is a method that extracts a single value from a tensor and returns it as a Python native type (like an integer or a float), but it does not return the tensor as it is.\n",
        "\n",
        "Typically, .item() is used when a tensor has only one value, such as when you're extracting a scalar from a tensor of shape (1,)\n",
        "\n",
        "But in the context of classification, .item() can be used for a different purpose: it’s used to get the index of the predicted class when dealing with logits or probabilities."
      ],
      "metadata": {
        "id": "YBtbdB3f38tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img,label=test_dataset[0]\n",
        "plt.imshow(img.permute(1,2,0))\n",
        "print('Label:',dataset.classes[label],'Predicted:',predict_image(img,model))"
      ],
      "metadata": {
        "id": "rz4Ke4SS1hZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img,label=test_dataset[1002]\n",
        "plt.imshow(img.permute(1,2,0))\n",
        "print('Label:',dataset.classes[label],'Predicted:',predict_image(img,model))"
      ],
      "metadata": {
        "id": "WBHcdu8146WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img,label=test_dataset[6153]\n",
        "plt.imshow(img.permute(1,2,0))\n",
        "print('Label:',dataset.classes[label],'Predicted:',predict_image(img,model))"
      ],
      "metadata": {
        "id": "bS4em-vu5QzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader=DeviceDataLoader(DataLoader(test_dataset,batch_size*2),device)\n",
        "result=evaluate(model,test_loader)\n",
        "result"
      ],
      "metadata": {
        "id": "L6kbeD6I5lxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),'cifar10-cnn.pth')"
      ],
      "metadata": {
        "id": "wY0lALnc577k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `.state_dict` method returns an `OrderedDict` containing all the weights and bias matrices mapped to the right attributes of the model. To load the model weights, we can redefine the model with the same structure, and use the `.load_state_dict` method."
      ],
      "metadata": {
        "id": "8xBEYKx16Rw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2=to_device(Cifar10CnnModel(),device)"
      ],
      "metadata": {
        "id": "djXjlskx6Svo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.load_state_dict(torch.load('cifar10-cnn.pth'))"
      ],
      "metadata": {
        "id": "l99iBaf96cNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model2,test_loader)"
      ],
      "metadata": {
        "id": "f_jjqRRn6tYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network architecture refers to the structure and design of a neural network, including the number of layers, the types of layers used, and how they are connected. It defines the model's ability to learn from data and make predictions or classifications."
      ],
      "metadata": {
        "id": "WoAgabXq7MNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BlogPosts"
      ],
      "metadata": {
        "id": "Kbvcb2hm63GQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}